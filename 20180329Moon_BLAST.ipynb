{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a system to auto-catagorize 16s rRNAs, ITS and blastn search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " content | location \n",
    " -------: | :--------\n",
    " blast+ folder | `/mnt/d/linux/P/blast/bin/`\n",
    " blastn | `/mnt/d/linux/P/blast/bin/blastn`\n",
    " ascp | `/mnt/d/linux/.aspera/connect/bin/ascp`\n",
    " ~ | `/mnt/d/linux`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ascp\n",
    "* `/mnt/d/linux/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/blast/db/16SMicrobial.tar.gz  ~/W/NCBI/16SMicriobial/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download taxonamy, gss, wgs, est\n",
    "\n",
    "run with linux\n",
    "```\n",
    "/mnt/d/linux/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/pub/taxonomy/accession2taxid/nucl_gss.accession2taxid.gz  ~/W/NCBI/taxonamy/\n",
    "\n",
    "/mnt/d/linux/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/pub/taxonomy/accession2taxid/nucl_wgs.accession2taxid.gz  ~/W/NCBI/taxonamy/\n",
    "\n",
    "/mnt/d/linux/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/pub/taxonomy/accession2taxid/nucl_est.accession2taxid.gz  ~/W/NCBI/taxonamy/\n",
    "\n",
    "/mnt/d/linux/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/pub/taxonomy/taxdump.tar.gz  ~/W/NCBI/taxonamy/\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download nt blast database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate the download code with Python\n",
    "```\n",
    "l = [\"/mnt/d/linux/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/blast/db/nt.%02d.tar.gz  ~/W/NCBI/nt201803/\"%i for i in range(56)]\n",
    "open(\"temp.txt\",\"w\").write(\"\\n\".join(l))\n",
    "```\n",
    "Then run the code stored in `temp.txt` in linux commandline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16SrRNAs of Microbial from NCBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### download 16SMicrobial.tar.gz from ncbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link address is ftp://ftp.ncbi.nlm.nih.gov/blast/db/16SMicrobial.tar.gz\n",
    "Download the file with ascp.\n",
    "\n",
    "The download code is as below. Run it in a ubuntu terminal.\n",
    "```\n",
    "/mnt/d/linux/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/blast/db/16SMicrobial.tar.gz  ~/W/NCBI/16SMicriobial/\n",
    "```\n",
    "\n",
    "decompress the file.\n",
    "```\n",
    "cd ~/W/NCBI/16SMicriobial/\n",
    "tar xzf 16SMicrobial.tar.gz\n",
    "```\n",
    "\n",
    "This is a database built for blast search. Extract fasta sequences to see how many sequences are there.\n",
    "```\n",
    "~/P/blast/bin/blastdbcmd -entry all -db 16SMicrobial -out 16SMicrobial.fasta\n",
    "```\n",
    "\n",
    "There are 19,757 sequences, 20,035 rRNA IDs. This should be a very good reference for 16S rRNAs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blast Kejing Wang's sequences against 16SMicrobial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`~/P/blast/bin/blastn -db ~/W/NCBI/16SMicriobial/16SMicrobial -query ~/W/MoonNt/20180327KC.fasta -out ~/W/MoonNt/20180329KC.tab -num_threads 7 -max_target_seqs 200 -outfmt '6 qseqid sseqid qcovhsp pident staxids  sscinames sskingdoms length mismatch gapopen qstart qend sstart send evalue bitscore'`\n",
    "\n",
    "The sseqid contains gi number and accession number, which looks like \"gi|343201646|ref|NR_042372.1|\". It is because when they built the database, gi number is still included.\n",
    "\n",
    "the output looks like"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MN210614\tgi|631252669|ref|NR_113867.1|\t100\t100.000\t93063\tSphingomonas aquatilis\t...\t2484\n",
    "MN210614\tgi|219857409|ref|NR_024997.1|\t100\t100.000\t93063\tSphingomonas aquatilis\t...\t2484\n",
    "MN210614\tgi|265678324|ref|NR_028626.1|\t100\t99.926\t152682\tSphingomonas melonis\t...\t2479"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-build the database so that gi number is excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-built 16SMicrobial Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get a Taxonomy id <–> sequence id file for 16SMicrobial\n",
    "\n",
    "```\n",
    "from Bio import SeqIO\n",
    "rna16s = set([e.id for e in SeqIO.parse(open(\"/mnt/d/linux/W/NCBI/16SMicriobial/16SMicrobial.fasta\"),format = \"fasta\")])\n",
    "acc2tax = open(\"/mnt/d/linux/W/NCBI/taxonamy/nucl_gb.accession2taxid\")\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/16SMicriobial/16SMicrobial.taxa\",\"w\", newline = \"\\n\")\n",
    "for line in acc2tax:\n",
    "    _es = line.split()\n",
    "    if _es[1] in rna16s:\n",
    "        fout.write(_es[1]+' ' + _es[2] + \"\\n\")\n",
    "fout.close()\n",
    "```\n",
    "checked, all 18757 sequences have taxonomy data available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build the database\n",
    "\n",
    "```\n",
    "~/P/blast/bin/makeblastdb -in /mnt/d/linux/W/NCBI/16SMicriobial/16SMicrobial.fasta -input_type fasta -dbtype nucl -parse_seqids -title \"16SMicrobial\" -out /mnt/d/linux/W/NCBI/16SMicriobial/16SMicrobial_DB  -taxid_map /mnt/d/linux/W/NCBI/16SMicriobial/16SMicrobial.taxa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blast Kejing Wang's sequences against newly built 16SMicrobial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`~/P/blast/bin/blastn -db ~/W/NCBI/16SMicriobial/16SMicrobial_DB -query ~/W/MoonNt/20180327KC.fasta -out ~/W/MoonNt/20180329KC.tab_n -num_threads 7 -max_target_seqs 200 -outfmt '6 qseqid sseqid qcovhsp pident staxids  sscinames sskingdoms length mismatch gapopen qstart qend sstart send evalue bitscore'`\n",
    "\n",
    "The sseqid contains gi number and accession number, which looks like \"gi|343201646|ref|NR_042372.1|\". It is because when they built the database, gi number is still included.\n",
    "\n",
    "the output looks like"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "MN210614\tref|NR_113867.1|\t100\t100.000\t93063\tSphingomonas aquatilis\t...\t2484\n",
    "MN210614\tref|NR_024997.1|\t100\t100.000\t93063\tSphingomonas aquatilis\t...\t2484\n",
    "MN210614\tref|NR_028626.1|\t100\t99.926\t152682\tSphingomonas melonis\t...\t2479"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "still not the form we want. Give up this test. Use the original Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate the species, genus, family, order, class, phylum table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### species, genus, family, order, class, phylum table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "file_node = \"/mnt/d/linux/W/NCBI/taxonamy/nodes.dmp\"\n",
    "import pandas as pd\n",
    "df_node = pd.read_csv(file_node, sep = \"\\t\",header = None,dtype = str)\n",
    "df_node = df_node.loc[:,[0,2,4]]\n",
    "df_node.columns = [\"From\",\"To\",\"rank\"]\n",
    "df_node.head()\n",
    "\n",
    "set(df_node.loc[:,\"rank\"])\n",
    "\n",
    "def f(x):\n",
    "    if x == \"subspecies\" or x == 'varietas':\n",
    "        return \"species\"\n",
    "    return x\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(df_node[\"rank\"]))\n",
    "df_node[\"rank\"] = df_node[\"rank\"].apply(f)\n",
    "print(Counter(df_node[\"rank\"]))\n",
    "\n",
    "\n",
    "\n",
    "ls_species = list(df_node[df_node[\"rank\"] == \"species\"].iloc[:,0])\n",
    "\n",
    "dc_node = dict(zip(list(df_node.loc[:,\"From\"]), list(df_node.loc[:,\"To\"])))\n",
    "dc_rank = dict(zip(list(df_node.loc[:,\"From\"]), list(df_node.loc[:,\"rank\"])))\n",
    "\n",
    "## added 20180531 ##\n",
    "## some 'no rank' rank is actually subspecies. \n",
    "def correct_no_rank(x):\n",
    "    '''\n",
    "    if rank == 'no rank', check if it is a subspecies. if it is, return 'species'\n",
    "    '''\n",
    "    if x['rank'] == 'no rank':\n",
    "        if dc_rank[x['To']] == 'species':\n",
    "            return 'species'\n",
    "        if dc_rank[dc_node[x['To']]] == 'species':\n",
    "            return 'species'\n",
    "    return x['rank']\n",
    "\n",
    "df_node['rank2'] = df_node.apply(correct_no_rank, axis=1)\n",
    "ls_species = list(df_node[df_node[\"rank2\"] == \"species\"].iloc[:,0])\n",
    "## end added 20180531 ##\n",
    "\n",
    "def find_lineage(species):\n",
    "    '''\n",
    "    given a species ID, return a list of species, genus, family, order, class and phylum id\n",
    "    '''\n",
    "    lineage = {}\n",
    "    From = species\n",
    "    rank_set = {'genus','family','order','class','phylum'}\n",
    "    lineage['species'] = species\n",
    "    while(True):\n",
    "        To = dc_node[From]\n",
    "        rank = dc_rank[From]\n",
    "        if From in {\"2\",\"2157\",\"12884\",\"10239\",\"4751\",\"33090\",\"33208\"}:\n",
    "            lineage[\"kingdom\"] = From\n",
    "        if From == To:\n",
    "            break\n",
    "        if rank in rank_set:\n",
    "            lineage[rank] = From\n",
    "            From = To\n",
    "        else:\n",
    "            From = To\n",
    "    return lineage\n",
    "\n",
    "ls_lineage = list(map(find_lineage,ls_species))\n",
    "df_lineage = pd.DataFrame(ls_lineage, dtype=str)\n",
    "df_lineage = df_lineage.loc[:,['species','genus','family','order','class','phylum',\"kingdom\"]]\n",
    "\n",
    "df_lineage.to_csv(\"/mnt/d/linux/W/NCBI/taxonamy/speicesLineage\", sep = \"\\t\", float_format = \"%d\", index = False)\n",
    "\n",
    "```\n",
    "\n",
    "kingdom:\n",
    "* Life\n",
    "    * Archae 2157\n",
    "    * Bacteria 2\n",
    "    * Viroids 12884\n",
    "    * Viruses 10239\n",
    "    * Eukaryota 2759\n",
    "        * Fungi 4751\n",
    "        * viridiplantae 33090\n",
    "        * metazoa 33208"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file looks like. For some species, some values are missing.\n",
    "\n",
    "species | genus | family | order | class | phylum | kingdom\n",
    "--------|-------|--------|-------|-------|----- |----\n",
    "7 | 6 | 335928 | 356 | 28211 | 1224 | 2\n",
    "9 | 32199 | 1903409 | 91347 | 1236 | 1224 | 2\n",
    "11 | 1707 | 85016 | 85006 | 1760 | 201174 | 2\n",
    "14 | 13 | 203488 | 203487 | 203486 | 68297 | 2\n",
    "17 | 16 | 32011 | 32003 | 28216 | 1224 | 2\n",
    "19 | 18 | 213421 | 69541 | 28221 | 1224 | 2\n",
    "21 | 20 | 76892 | 204458 | 28211 | 1224 | 2\n",
    "23 | 22 | 267890 | 135622 | 1236 | 1224 | 2\n",
    "24 | 22 | 267890 | 135622 | 1236 | 1224 | 2\n",
    "25 | 22 | 267890 | 135622 | 1236 | 1224 | 2\n",
    "27 |  |  |  |  |  | 2\n",
    "28 |  |  |  |  |  | 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lineage ID and scientific name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the file \"speciesLineage\" coding in numbers.\n",
    "Generate a file with lineage ID and scientific names.\n",
    "```\n",
    "file_name = \"/mnt/d/linux/W/NCBI/taxonamy/names.dmp\"\n",
    "import pandas as pd\n",
    "df_name = pd.read_csv(file_name,sep = \"\\t\", header = None,dtype = str)\n",
    "\n",
    "df_name = df_name.loc[:,[0,2,6]]\n",
    "df_name_keep = df_name[df_name[6] == \"scientific name\"]\n",
    "\n",
    "df_name_keep = df_name_keep.loc[:,[0,2]]\n",
    "df_name_keep.columns = [\"id\",\"name\"]\n",
    "df_name_keep.to_csv(\"/mnt/d/linux/W/NCBI/taxonamy/speciesName\", sep = \"\\t\", float_format = \"%d\", index = False)\n",
    "```\n",
    "\n",
    "File looks like:\n",
    "```\n",
    "id\tname\n",
    "1\troot\n",
    "2\tBacteria\n",
    "6\tAzorhizobium\n",
    "7\tAzorhizobium caulinodans\n",
    "9\tBuchnera aphidicola\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lineage ID and lineage names\n",
    "\n",
    "change speicesLineage to speicesLineageName, with two column, taxid and species lineage\n",
    "```\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "file_Name = \"/mnt/d/linux/W/NCBI/taxonamy/speciesName\"\n",
    "file_Lineage = \"/mnt/d/linux/W/NCBI/taxonamy/speicesLineage\"\n",
    "\n",
    "df_Lineage = pd.read_csv(file_Lineage,sep=\"\\t\",dtype=str)\n",
    "\n",
    "dcName = {}\n",
    "for _l in open(file_Name):\n",
    "    _k,_v = _l[:-1].split(\"\\t\")\n",
    "    dcName[_k] = _v\n",
    "\n",
    "df_LineageName = df_Lineage.copy()\n",
    "df_LineageName.index = df_LineageName[\"species\"].copy()\n",
    "df_LineageName.index.name = \"taxID\"\n",
    "df_LineageName = df_LineageName.applymap(lambda x: dcName[x] if x in dcName else \"\")\n",
    "df_LineageName.to_csv(\"/mnt/d/linux/W/NCBI/taxonamy/speicesLineageName\",sep=\"\\t\")\n",
    "\n",
    "df_head = df_LineageName.apply(lambda x:\";\".join(x),axis=1)\n",
    "\n",
    "open(\"/mnt/d/linux/W/NCBI/taxonamy/speicesLineage.head\",\"w\").write(''.join(df_LineageName.apply(lambda x:x.name+'\\t'+';'.join(x)+'\\n',axis=1)))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check if all 16SMicrobial have full taxonomy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "templs = open(\"/mnt/d/linux/W/NCBI/16SMicriobial/16SMicrobial.taxa\").readlines()\n",
    "templs = [e.split() for e in templs]\n",
    "taxa_16S = [e[1] for e in templs]\n",
    "taxa_16S = set(taxa_16S)\n",
    "id_16S = [e[0] for e in templs]\n",
    "\n",
    "fileLineage = \"/mnt/d/linux/W/NCBI/taxonamy/speicesLineage\"\n",
    "ls = open(fileLineage).readlines()\n",
    "ls = ls[1:]\n",
    "ls = [e.replace(\"\\n\",\"\") for e in ls]\n",
    "ls = [e.split(\"\\t\") for e in ls]\n",
    "dc = {e[0]:e for e in ls if e.count(\"\") == 0}\n",
    "print(len([e for e in taxa_16S if e not in dc]))\n",
    "\n",
    "good_taxa16S = [e for e in taxa_16S if e in dc]\n",
    "dc_good = {e:dc[e] for e in good_taxa16S}\n",
    "#dc_good['173959']\n",
    "\n",
    "ls = open(\"/mnt/d/linux/W/NCBI/taxonamy/speciesName\").readlines()\n",
    "ls = ls[1:]\n",
    "ls = [e.replace(\"\\n\",\"\") for e in ls]\n",
    "ls = [e.split(\"\\t\") for e in ls]\n",
    "dcName = {e[0]:e[1] for e in ls}\n",
    "\n",
    "dc_goodName = {e:[dcName[i] for i in dc_good[e]] for e in dc_good}\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/16SMicriobial/16SMicrobial.Lineage\",\"w\")\n",
    "fout.write(\"id\\tspecies\\tgenus\\tfamily\\torder\\tclass\\tphylum\\n\")\n",
    "for e in dc_goodName:\n",
    "    fout.write(e + \"\\t\" + \"\\t\".join(dc_goodName[e]) + \"\\n\")\n",
    "fout.close()\n",
    "```\n",
    "\n",
    "1893 out of 14875 species in 16SMicrobial do not have \"perfect\" taxonomy terms. \n",
    "Generate a file for usage of these 12982 species directly.\n",
    "The file is saved as \"16SMicrobial.Lineage\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blast against NCBI nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blast Kejing Wang's sequences against nt\n",
    "\n",
    "`~/P/blast/bin/blastn -db ~/W/NCBI/nt/nt -query ~/W/MoonNt/20180327KC.fasta -out ~/W/MoonNt/20180329KC_nt.tab -num_threads 7 -max_target_seqs 200 -outfmt '6 qseqid sseqid qcovhsp pident staxids  sscinames sskingdoms length mismatch gapopen qstart qend sstart send evalue bitscore'`\n",
    "\n",
    "Just to check the speed of blast nt.\n",
    "\n",
    "This step is extremely slow. We need to reduce the size of database to speed up the process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare rRNA database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract all ribosome-related sequences\n",
    "\n",
    "rRNA sequences usually have \"ribosomal RNA\" or \"rRNA\" as key words in the headline of the fasta file. To include all ribosome-related sequences, use \"ribosom\" and \"rRNA\" as key words to extract them all.\n",
    "\n",
    "```\n",
    "filename = \"/mnt/d/linux/W/NCBI/nt/nt\"\n",
    "fo = open(filename)\n",
    "import re\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA\",\"w\")\n",
    "for line in fo:\n",
    "    if line[0] == \">\":\n",
    "        if re.search(\"ribosom\",line,re.I) or re.search(\"rRNA\",line,re.I):\n",
    "            fout.write(line)\n",
    "            rRNA = True\n",
    "        else:\n",
    "            rRNA = False\n",
    "    else:\n",
    "        if rRNA:\n",
    "            fout.write(line)\n",
    "fout.close()\n",
    "fo.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is too big for hs-blastn. Not enough memory in the 32GB PC. Further extract 16s RNA sequences using other filter parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a database rRNA, hs-blastn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /mnt/d/linux/W/NCBI/nt_rRNA/\n",
    "~/P/blast/bin/makeblastdb -in rRNA -input_type fasta -dbtype nucl\n",
    "~/P/blast/bin/windowmasker -in rRNA -infmt blastdb -mk_counts -out rRNA.counts\n",
    "~/P/blast/bin/windowmasker -in rRNA.counts -sformat obinary -out rRNA.counts.obinary -convert\n",
    "~/P/hs-blastn-src/hs-blastn index rRNA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter rRNA sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The average lengths of the structural rRNA genes are 1,522 bp, 2,971 bp, and 120 bp respectively for 16S, 23S, and 5S rRNAs.\n",
    " Out of 19757 seqs in 16sMicrobial, only 19 is shorter than 1000 bp. 10 is longer than 1800.\n",
    " remove sequences with more than ten \"N\"\n",
    " \n",
    " **1,964,372** sequences in rRNA2\n",
    "```\n",
    "from Bio import SeqIO\n",
    "lsIn = SeqIO.parse( open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA\"), format=\"fasta\")\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2\", \"w\")\n",
    "for s in lsIn:\n",
    "    if len(s.seq) >= 1000 and len(s.seq) <= 1800 and (str(s.seq).count(\"N\") + str(s.seq).count(\"n\")) < 10:\n",
    "        fout.write(\">\"+s.description+\"\\n\"+str(s.seq)+\"\\n\")\n",
    "fout.close()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "~/P/blast/bin/blastn -db /mnt/d/linux/W/NCBI/16SMicriobial/16SMicrobial -query /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2 -outfmt 6 -out /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.blast16S.tab -num_alignments 1 -word_size 28 -num_threads 8\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "blastn is slow. Go back to hs-blastn\n",
    "```\n",
    "cd /mnt/d/linux/W/NCBI/16SMicriobial/\n",
    "~/P/hs-blastn-src/hs-blastn align -db 16SMicrobial.fasta -window_masker_db 16SMicrobial.fasta.counts.obinary -query /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2 -out /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.16S.tab.HS -num_threads 8 -max_target_seqs 1 -outfmt 6 -word_size 28\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter those with alignments matched length >800, identity > 80%.\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.16S.tab.HS\", header = None, sep = \"\\t\")\n",
    "df1 = df[(df[2] > 80) & (df[3] > 800)]\n",
    "possible_16S = set(df1[0])\n",
    "\n",
    "from Bio import SeqIO\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA3\", \"w\")\n",
    "seqs = SeqIO.parse(open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2\"),\"fasta\")\n",
    "for seq in seqs:\n",
    "    if seq.id in possible_16S:\n",
    "        fout.write(\">\"+seq.description+\"\\n\"+str(seq.seq)+\"\\n\")\n",
    "fout.close()\n",
    "```\n",
    "\n",
    "1,542,730 sequences in rRNA3 file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a database for filtered rRNA, hs-blastn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /mnt/d/linux/W/NCBI/nt_rRNA/\n",
    "~/P/blast/bin/makeblastdb -in rRNA3 -input_type fasta -dbtype nucl\n",
    "~/P/blast/bin/windowmasker -in rRNA3 -infmt blastdb -mk_counts -out rRNA3.counts\n",
    "~/P/blast/bin/windowmasker -in rRNA3.counts -sformat obinary -out rRNA3.counts.obinary -convert\n",
    "~/P/hs-blastn-src/hs-blastn index rRNA3\n",
    "```\n",
    "\n",
    "The output of hs-blastn is not good. The input file contains 255 sequences, while the output file contains 158 query ids. This problem cannot be solved by using the recommended blast+2.3.0. Give up all test best on hs-blastn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a database for rRNA2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare the taxid_map file. Note, taxid_map, use accession number without the version number.\n",
    "```\n",
    "from Bio import SeqIO\n",
    "seqs = SeqIO.parse(open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2\"),format=\"fasta\")\n",
    "names = set([e.id for e in seqs])\n",
    "taxa = open(\"/mnt/d/linux/W/NCBI/taxonamy/nucl_gb.accession2taxid\")\n",
    "taxa_rRNA2 = open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.taxa\",\"w\")\n",
    "for line in taxa:\n",
    "    e1,e2, e3, e4 = line.split()\n",
    "    if e2 in names:\n",
    "        taxa_rRNA2.write(e1 + \" \" + e3 + \"\\n\")\n",
    "taxa_rRNA2.close()\n",
    "```\n",
    "build the database\n",
    "\n",
    "```\n",
    "/mnt/d/linux/P/blast/bin/dustmasker -in /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2 -infmt fasta -parse_seqids -outfmt maskinfo_asn1_bin -out /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.asnb\n",
    "\n",
    "/mnt/d/linux/P/blast/bin/makeblastdb -in /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2 -input_type fasta -dbtype nucl -mask_data /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.asnb -out /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2 -title \"rRNA2\" -parse_seqids -taxid_map /mnt/d/linux/W/NCBI/rRNA2.taxa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### blast Kejing Wang's seq against nt rRNA3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /mnt/d/linux/W/NCBI/nt_rRNA/\n",
    "~/P/hs-blastn-src/hs-blastn align -db rRNA3 -window_masker_db rRNA3.counts.obinary -query ~/W/MoonNt/20180327KC.fasta -out ~/W/MoonNt/20180330KC_ntRNA.tab -num_threads 8 -max_target_seqs 200 -outfmt 6 -word_size 12\n",
    "\n",
    "\n",
    "~/P/blast/bin/blastn -db rRNA3 -query ~/W/MoonNt/20180327KC.fasta -out ~/W/MoonNt/20180330KC_ntRNA.tab_blastn -num_threads 7 -max_target_seqs 200 -outfmt 6\n",
    "```\n",
    "\n",
    "For hs-blastn search, use `-word_size 12` to generate similar result as `blastn`. The speed is very good. The result is still a little different compared to `blastn`. Design the pipleline carefully. \n",
    "\n",
    "First, use blastn against 16SMicrobial and hs-blastn against rRNA3. If no significant different difference for the best match, use 16SMicrobial results. Else, run blastn against rRNA3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract sequence length, header, and taxomany id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate 3 files. rRNA2.len, rRNA2.head, rRNA2.taxa, rRNA2.rare.\n",
    "\n",
    "```\n",
    "filename = \"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2\"\n",
    "\n",
    "\n",
    "from Bio import SeqIO\n",
    "f_silva = filename\n",
    "seqs = SeqIO.parse(open(f_silva),\"fasta\")\n",
    "fout1 = open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.len\",\"w\")\n",
    "fout2 = open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.head\",\"w\")\n",
    "accs = []\n",
    "for seq in seqs:\n",
    "    fout1.write(seq.id + \"\\t\" + str(len(seq.seq)) + \"\\n\")\n",
    "    fout2.write(seq.id + \"\\t\" +seq.description.split(\" \",maxsplit=1)[1] + \"\\n\")\n",
    "    accs.append(seq.id)\n",
    "fout1.close()\n",
    "fout2.close()\n",
    "\n",
    "accs = set(accs)\n",
    "accs_v = list(accs)\n",
    "accs = set([e.split(\".\")[0] for e in accs_v])\n",
    "print(\"accs length\", len(accs))\n",
    "\n",
    "#creat a dictionary with accession number as key, tax id as value\n",
    "dc_acc2tax = {}\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_gb.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_gss.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_wgs.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_est.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "accsNotInNCBI = [e for e in accs if e not in dc_acc2tax]\n",
    "print(len(accsNotInNCBI))\n",
    "\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.taxa\",\"w\")\n",
    "fout2 = open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.taxaRare\",\"w\")\n",
    "for seq_acc in accs:\n",
    "    if seq_acc in dc_acc2tax:\n",
    "        fout.write(seq_acc + \"\\t\" + dc_acc2tax[seq_acc] +\"\\n\")\n",
    "    else:\n",
    "        fout2.write(seq_acc + \"\\n\")\n",
    "fout.close()\n",
    "fout2.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SILVA 16S rRNA database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SILVA rRNA database project\n",
    "A comprehensive on-line resource for quality checked and aligned ribosomal RNA sequence data.\n",
    "I downloaded 16S rRNA database from the link `https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/SILVA_132_SSURef_tax_silva.fasta.gz`\n",
    "\n",
    "There are **2,090,688** sequences. \n",
    "\n",
    "The titles of the sequences are `JQ771801.1.1459 Bacteria;Actinobacteria;Actinobacteria;Streptomycetales;Streptomycetaceae;Streptomyces;Streptomyces sp. Ac105`. \n",
    "* `JQ771801` accession number of NCBI\n",
    "* `1.1459` the location of rRNA in the sequence\n",
    "* `Bacteria;Actinobacteria;Actinobacteria;Streptomycetales;Streptomycetaceae;Streptomyces;Streptomyces sp. Ac105`: kingdom;phylum;class;order;family;genus;species\n",
    "\n",
    "check if all sequences have titles shown above. It turns out the taxonamy is not always listed as above (which contains 7 parts). It can be like `Eukaryota;Opisthokonta;Holozoa;Metazoa (Animalia);Eumetazoa;Bilateria;Chordata;Vertebrata;Gnathostomata;Euteleostomi;Tetrapoda;Mammalia;Homo sapiens (human)`, which means it contains full lineage. \n",
    "```\n",
    "from Bio import SeqIO\n",
    "f_silva = \"/mnt/d/linux/W/NCBI/SILVA/SILVA_132_SSURef_tax_silva.fasta\"\n",
    "seqs = list(SeqIO.parse(open(f_silva),\"fasta\"))\n",
    "titles = [e.description for e in seqs]\n",
    "import re\n",
    "re.match(\"^\\w*\\.\\d*\\.\\d* \\w*;([a-zA-Z 0-9]*;){6}$\",titles[0])\n",
    "nogood = [e for e in titles if not re.match(\"^\\w*\\.\\d*\\.\\d* \\w*;([a-zA-Z 0-9]*;){5}([a-zA-Z 0-9]*)$\",e)]\n",
    "nogood2 = [e for e in titles if not re.match(\"^\\w*\\.\\d*\\.\\d* \\w*;([^;]*;){5}([a-zA-Z 0-9]*)$\",e)]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2,090,688 sequences, form **2,040,731** accession numbers. **5244** of them do not have good taxonamy ids.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to DNA\n",
    "\n",
    "Convert sequences of RNA to DNA.\n",
    "\n",
    "```\n",
    "from Bio import SeqIO\n",
    "f_silva = \"/mnt/d/linux/W/NCBI/SILVA/SILVA_132_SSURef_tax_silva.fasta\"\n",
    "seqs = SeqIO.parse(open(f_silva),\"fasta\")\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.fasta\",\"w\")\n",
    "for seq in seqs:\n",
    "    fout.write(\">\" + seq.description + \"\\n\" + str(seq.seq).upper().replace(\"U\",\"T\")+\"\\n\")\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract seq length\n",
    "\n",
    "Write a file with seq length\n",
    "`JQ771801.1.1459 1459`\n",
    "\n",
    "```\n",
    "from Bio import SeqIO\n",
    "f_silva = \"/mnt/d/linux/W/NCBI/SILVA/SILVA_132_SSURef_tax_silva.fasta\"\n",
    "seqs = SeqIO.parse(open(f_silva),\"fasta\")\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.len\",\"w\")\n",
    "for seq in seqs:\n",
    "    fout.write(seq.id + \"\\t\" + str(len(seq.seq)) + \"\\n\")\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract seq header\n",
    "\n",
    "`JQ771801.1.1459 Bacteria;Actinobacteria;Actinobacteria;Streptomycetales;Streptomycetaceae;Streptomyces;Streptomyces sp. Ac105`\n",
    "\n",
    "```\n",
    "from Bio import SeqIO\n",
    "f_silva = \"/mnt/d/linux/W/NCBI/SILVA/SILVA_132_SSURef_tax_silva.fasta\"\n",
    "seqs = SeqIO.parse(open(f_silva),\"fasta\")\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.head\",\"w\")\n",
    "for seq in seqs:\n",
    "    fout.write(seq.id + \"\\t\" +seq.description.split(\" \",maxsplit=1)[1] + \"\\n\")\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract taxonamy id\n",
    "\n",
    "Write a file with seqID, accession number, length, taxonamy. The separater is \"\\t\"\n",
    "`JQ771801 taxID`. \n",
    "If seqID do not have taxID, write another file\n",
    "`JQ771801 Bacteria;Actinobacteria;Actinobacteria;Streptomycetales;Streptomycetaceae;Streptomyces;Streptomyces sp. Ac105`\n",
    "\n",
    "```\n",
    "from Bio import SeqIO\n",
    "f_silva = \"/mnt/d/linux/W/NCBI/SILVA/SILVA_132_SSURef_tax_silva.fasta\"\n",
    "seqs = list(SeqIO.parse(open(f_silva),\"fasta\"))\n",
    "dcseqs = {e.id.split(\".\")[0]:e for e in seqs}\n",
    "#get the accession ids\n",
    "accs = set([e.id.split(\".\")[0] for e in seqs])\n",
    "print(len(accs))\n",
    "\n",
    "#get a dictionary of seqs\n",
    "dcseqs = {e.id.split(\".\")[0]:e for e in seqs}\n",
    "\n",
    "\n",
    "#creat a dictionary with accession number as key, tax id as value\n",
    "dc_acc2tax = {}\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_gb.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_gss.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_wgs.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_est.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "accsNotInNCBI = [e for e in accs if e not in dc_acc2tax]\n",
    "print(len(accsNotInNCBI))\n",
    "\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.taxa\",\"w\")\n",
    "fout2 = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.taxaRare\",\"w\")\n",
    "for seq_acc in dcseqs:\n",
    "    if seq_acc in dc_acc2tax:\n",
    "        fout.write(seq_acc + \"\\t\" + dc_acc2tax[seq_acc] +\"\\n\")\n",
    "    else:\n",
    "        fout2.write(seq_acc + \"\\t\" + dcseqs[seq_acc].description.split(\" \",maxsplit = 1)[1] + \"\\n\")\n",
    "fout.close()\n",
    "fout2.close()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build blast db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /mnt/d/linux/W/NCBI/SILVA/\n",
    "/mnt/d/linux/P/blast/bin/dustmasker -in SILVA16sDNA.fasta -infmt fasta -parse_seqids -outfmt maskinfo_asn1_bin -out SILVA16sDNA.asnb\n",
    "\n",
    "/mnt/d/linux/P/blast/bin/makeblastdb -in SILVA16sDNA.fasta -input_type fasta -dbtype nucl -mask_data SILVA16sDNA.asnb -out SILVA16sDNA -title \"SILVA16s\" -parse_seqids -taxid_map SILVA16sDNA.taxa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a file for species with good lineage info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "speicesLineage, only keep those with five elements per line.\n",
    "For taxonamy, speciesName, remove those contain the keywords \"environment\", \"unidentified\", \"uncultured\". Generate a file.\n",
    "of the six terms \"species\tgenus\tfamily\torder\tclass\tphylum\", allow missing two of them.\n",
    "\n",
    "Filename **SILVA16sDNA.goodLineage**\n",
    "\n",
    "```\n",
    "f_Lineage = \"/mnt/d/linux/W/NCBI/taxonamy/speicesLineage\"\n",
    "ls_Lineage = open(f_Lineage).readlines()\n",
    "\n",
    "ls_Lineage = [e.split() for e in ls_Lineage]\n",
    "ls_LineageGood = [e for e in ls_Lineage if len(e) >= 5]\n",
    "ls_LineageGood2 = [e for e in ls_Lineage if len(e) >= 4]\n",
    "\n",
    "ls_LineageGoodSpecies = [e[0] for e in ls_LineageGood2]\n",
    "st_goodSpecies = set(ls_LineageGoodSpecies[1:])\n",
    "\n",
    "f_names = \"/mnt/d/linux/W/NCBI/taxonamy/speciesName\"\n",
    "import re\n",
    "ls_names = open(f_names).readlines()[1:]\n",
    "ls_namesSpecies = [e for e in ls_names if e.split()[0] in st_goodSpecies]\n",
    "ls_goodnames = []\n",
    "noGood = 0\n",
    "for e in ls_namesSpecies:\n",
    "    if re.search(\"environment\", e, re.I) or re.search(\"unidentified\", e, re.I) or re.search(\"uncultured\", e, re.I):\n",
    "        noGood += 1\n",
    "    else:\n",
    "        ls_goodnames.append(e)\n",
    "print(len(ls_goodnames))\n",
    "print(noGood)\n",
    "\n",
    "ls_goodSpecies = [e.split()[0] for e in ls_goodnames]\n",
    "st_goodSpeciesName = set(ls_goodSpecies) #species not with good name and lineage \n",
    "\n",
    "ls_speciesSILVA = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.taxa\").readlines()\n",
    "ls_speciesSILVA = [e.split()[1] for e in ls_speciesSILVA]\n",
    "st_speciesSILVA = set(ls_speciesSILVA)\n",
    "\n",
    "#check keep SILVA species in st_goodSpeciesName\n",
    "st_speciesSILVAgoodLineage = st_speciesSILVA & st_goodSpeciesName\n",
    "#write the lineage info for st_speciesSILVAgoodLineage\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.goodLineage\",\"w\")\n",
    "fout.write(\"\\t\".join([\"taxID\",\"species\",\"genus\",\"family\",\"order\",\"class\",\"phylum\",\"kingdom\"]) +\"\\n\")\n",
    "dc_Lineage = {}\n",
    "lstemp = open(f_Lineage).readlines()[1:]\n",
    "for e in lstemp:\n",
    "    es = e[:-1].split(\"\\t\")\n",
    "    dc_Lineage[es[0]] = es\n",
    "dc_names = {e[:-1].split(\"\\t\")[0]:e[:-1].split(\"\\t\")[1] for e in ls_names}\n",
    "dc_names[\"\"] = \"\"\n",
    "\n",
    "for e in st_speciesSILVAgoodLineage:\n",
    "    _l = dc_Lineage[e]\n",
    "    _lnames = [dc_names[i] for i in _l]\n",
    "    fout.write(\"\\t\".join([e] + _lnames) +\"\\n\")\n",
    "fout.close()    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build a database with good taxa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sequences in SILVA and rRNA2, extract those with good lineage Info.   \n",
    "starting from`species` and have >3 out of 5 terms in`genus, family, order, class, phylum`\n",
    "\n",
    "Conclusion: not necessary. The \"good taxa\" database is still pretty large. Use SILVA\n",
    "\n",
    "```\n",
    "f_Lineage = \"/mnt/d/linux/W/NCBI/taxonamy/speicesLineage\"\n",
    "import pandas as pd\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "df_lineage = pd.read_csv(f_Lineage,sep = \"\\t\", dtype = str)\n",
    "df_lineage[\"taxaCount\"] = df_lineage.apply(lambda x:6-sum(x.isnull()), axis = 1)\n",
    "df_lineageFilter = df_lineage[df_lineage[\"taxaCount\"] >=4]\n",
    "\n",
    "species = set(df_lineageFilter[\"species\"])\n",
    "\n",
    "f_names = \"/mnt/d/linux/W/NCBI/taxonamy/speciesName\"\n",
    "df_names = pd.read_csv(f_names,sep=\"\\t\",dtype=str)\n",
    "df_names[\"good\"] = df_names[\"name\"].apply(lambda e:False if re.search(\"environment\", e, re.I) or re.search(\"unidentified\", e, re.I) or re.search(\"uncultured\", e, re.I) else True)\n",
    "\n",
    "speciesGoodname = set(df_names[df_names[\"good\"]][\"id\"])\n",
    "\n",
    "speciesGoodLineageName = species & speciesGoodname\n",
    "\n",
    "accessionGood = []\n",
    "f = \"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.taxa\"\n",
    "for line in open(f):\n",
    "    _acc, _species = line.split()\n",
    "    if _species in speciesGoodLineageName:\n",
    "        accessionGood.append(_acc)\n",
    "f = \"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2.taxa\"\n",
    "for line in open(f):\n",
    "    _acc, _species = line.split()\n",
    "    if _species in speciesGoodLineageName:\n",
    "        accessionGood.append(_acc)\n",
    "\n",
    "accessionGoodst = set(accessionGood)\n",
    "\n",
    "#get fasta with acc in accessionGoodst\n",
    "dcgoodfasta = {}\n",
    "\n",
    "f = \"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA2\"\n",
    "for _s in SeqIO.parse(open(f),format=\"fasta\"):\n",
    "    if _s.id.split(\".\")[0] in accessionGoodst:\n",
    "        dcgoodfasta[_s.id.split(\".\")[0]] = _s\n",
    "print(len(dcgoodfasta))\n",
    "f = \"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.fasta\"\n",
    "for _s in SeqIO.parse(open(f),format=\"fasta\"):\n",
    "    if _s.id.split(\".\")[0] in accessionGoodst:\n",
    "        dcgoodfasta[_s.id.split(\".\")[0]] = _s\n",
    "print(len(dcgoodfasta))\n",
    "\n",
    "\n",
    "lsgoodfasta = list(dcgoodfasta.values())\n",
    "runfile(\"/mnt/c/Users/ATPs/Documents/GitHub/XCProject/fasta/largeFastaFile.py\")\n",
    "lsunique = fasta_uni_keepone(lsgoodfasta)\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/nt_rRNA/rRNA_reference\",\"w\")\n",
    "for e in lsunique:\n",
    "    fout.write(\">\"+e.description+\"\\n\"+str(e.seq)+\"\\n\")\n",
    "fout.close()\n",
    "\n",
    "f = \"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.fasta\"\n",
    "_ls = open_fasta_to_list(f)\n",
    "print(len(_ls))\n",
    "_lsuni = fasta_uni_keepone(_ls)\n",
    "print(len(_lsuni))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build the pipeline 16S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "of the 255 sequences for Kejing Wang, \n",
    "* 7 of them are identical. \n",
    "* After removing sequences which are completely part of the other one, 179 sequences left. \n",
    "* If we allow the matched error rate to 0.001, 167 sequences left.\n",
    "* If we allow the matched error rate to 0.005, 144 sequences left.\n",
    "* If we allow the matched error rate to 0.010, 130 sequences left.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Mar 30 13:35:56 2018\n",
    "\n",
    "@author: x\n",
    "\"\"\"\n",
    "\n",
    "folder = \"/mnt/d/linux/W/MoonNt/KC/\"\n",
    "error_rate = 0.005\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "def getSeqFiles(folder):\n",
    "    '''\n",
    "    given a foler name, return a list of all sequence files.\n",
    "    seq files are files end with .seq\n",
    "    '''\n",
    "    files = glob.glob(folder + '**/*.seq', recursive=True)\n",
    "    return files\n",
    "\n",
    "def getFastaFromFile(filename):\n",
    "    '''\n",
    "    given a filename, return a str of fasta format.\n",
    "    some files contain extra info\n",
    "    filename looks like '/mnt/d/linux/W/MoonNt/KC/180223 SEQ/MN210615.seq'\n",
    "    return a string of fasta format\n",
    "    '''\n",
    "    fasta_str = \">\" + re.split(\"\\\\\\\\|/\",filename)[-1].split(\".\")[0] + \"\\n\"\n",
    "    txt_ls = open(filename).read().split(\"\\n\")\n",
    "    if txt_ls[0][0] != \">\":\n",
    "        seq = [e for e in txt_ls if re.match(\"[ATCGN]+\\n?$\",e,re.I)]\n",
    "        if len(seq) == 0:\n",
    "            print(filename, \"something is wrong\")\n",
    "            return \"\"\n",
    "        return fasta_str + ''.join(seq) + \"\\n\"\n",
    "    return fasta_str + str(SeqIO.read(open(filename),format = \"fasta\").seq) + \"\\n\"\n",
    "\n",
    "def taxaFinder(folder = folder):\n",
    "    '''\n",
    "    given a path of a folder, output a file with the alignment and taxa info for the sequences\n",
    "    '''\n",
    "    # add the slash to make the path easier to use\n",
    "    if folder[-1] != \"/\":\n",
    "        folder = folder + \"/\"\n",
    "    \n",
    "    # get paths of all sequence files\n",
    "    files = getSeqFiles(folder)\n",
    "    \n",
    "    # get all fasta sequences\n",
    "    fasta_seqs = [getFastaFromFile(e) for e in files]\n",
    "    \n",
    "    # save the fasta_seqs to a file in folder\n",
    "    open(folder + \"AllSeqs.fasta\",\"w\").write(\"\".join(fasta_seqs))\n",
    "    \n",
    "    # run local blast against SILVA\n",
    "    command_line = \"/mnt/d/linux/P/blast/bin/blastn -db /mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA -query {folder}AllSeqs.fasta  -out {folder}AllSeqs.fasta.SILVA.tab -num_threads 8 -max_target_seqs 100 -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovhsp staxids'\".format(folder = folder)\n",
    "    subprocess.run(command_line, shell=True)\n",
    "    \n",
    "    # read in blast result for SILVA\n",
    "    df_SILVA = pd.read_csv(\"{folder}AllSeqs.fasta.SILVA.tab\".format(folder = folder), sep = \"\\t\",header = None, dtype = {0:str,1:str,2:float,3:int,4:int,5:int,6:int,7:int,8:int,9:int,10:float,11:float,12:int,13:str})\n",
    "    df_SILVA.columns = [\"query\", \"subject\",\"identity\",\"matchLength\",\"missMatch\",\"gap\",\"qstart\",\"qend\",\"sstart\", \"send\", \"evalue\", \"bitScore\", \"qcover\", \"taxID\"]\n",
    "    ## add column 14, length of identical bases\n",
    "    df_SILVA[\"identical\"] = round(df_SILVA[\"identity\"] * df_SILVA[\"matchLength\"]/100)\n",
    "    ## for subject id, only keep accession number\n",
    "    df_SILVA[\"subject\"] = df_SILVA[\"subject\"].apply(lambda x: x.split(\".\")[0])\n",
    "    ## sort by query id and identical bases\n",
    "#    df_SILVA = df_SILVA.sort_values(by = [0,14],axis = 0, ascending=[True, False])\n",
    "    ## get max identical bases for each query\n",
    "#    query_max = df_SILVA.groupby(by = [0])[14].max()\n",
    "#    query_max = query_max.to_dict()\n",
    "#    ## add column 15, allow the identical bases to with the error of 0.5% compare to query_max\n",
    "#    df_SILVA[15] = df_SILVA.apply(lambda x:(query_max[x[0]] - x[14])/query_max[x[0]] <= error_rate, axis = 1)\n",
    "#    ## identical bases True \n",
    "#    df_SILVA_filter1 = df_SILVA[df_SILVA[15]]\n",
    "    \n",
    "    ## add a column, to filter identity > 97, qcover > 95\n",
    "    df_SILVA[\"filter1\"] = (df_SILVA[\"identity\"] > 97) & (df_SILVA[\"qcover\"] > 95)\n",
    "    \n",
    "    df_Lineage = pd.read_csv(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.head\", sep = \"\\t\", header = None, dtype = str)\n",
    "    df_Lineage.columns = [\"acc\",\"SILVAtaxa\"]\n",
    "    df_Lineage[\"acc\"] = df_Lineage[\"acc\"].apply(lambda x:x.split(\".\")[0])\n",
    "    df_Lineage.index = df_Lineage[\"acc\"]\n",
    "    df_Lineage = df_Lineage.drop(labels=[\"acc\"], axis = 1)\n",
    "    \n",
    "    df_SILVA = df_SILVA.join(df_Lineage, on=\"subject\",how=\"left\")\n",
    "    \n",
    "    \n",
    "    # read in SILVA16sDNA.goodLineage as dataframe\n",
    "    df_goodLineage = pd.read_csv(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.goodLineage\", sep = \"\\t\", dtype = str)\n",
    "    df_goodLineage.index = df_goodLineage[\"taxID\"]\n",
    "    df_goodLineage = df_goodLineage.drop(labels=[\"taxID\"], axis = 1)\n",
    "    \n",
    "    df_SILVA = df_SILVA.join(df_goodLineage, on=\"taxID\",how=\"left\")\n",
    "    df_SILVA.loc[:,['query', 'subject', 'identity', 'matchLength', 'qcover',\n",
    "       'taxID', 'identical', 'species', 'genus',\n",
    "       'family', 'order', 'class', 'phylum', 'SILVAtaxa']].to_excel(folder + \"AllSILVAresult.xlsx\", index = False)\n",
    "    \n",
    "    df_SILVA_keep1 = df_SILVA[df_SILVA[\"filter1\"] & df_SILVA[\"species\"] & df_SILVA[\"taxID\"]].drop_duplicates(subset = [\"query\"], keep = \"first\").copy()\n",
    "    \n",
    "    df_SILVA_keep1.loc[:,['query', 'subject', 'identity', 'matchLength', 'qcover',\n",
    "       'taxID', 'identical', 'species', 'genus',\n",
    "       'family', 'order', 'class', 'phylum', 'SILVAtaxa']].to_excel(folder + \"BestSILVAresult.xlsx\", index = False)\n",
    "    \n",
    "    queries = set(df_SILVA_keep1[\"query\"])\n",
    "    fout = open(folder + \"sequencesNotIdentified.fasta\",\"w\")\n",
    "    for seq in fasta_seqs:\n",
    "        _id = seq.split()[0][1:]\n",
    "        if _id not in queries:\n",
    "            fout.write(seq)\n",
    "    fout.close()\n",
    "    \n",
    "#    command_line = \"/mnt/d/linux/P/blast/bin/blastn -db /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2 -query {folder}AllSeqs.fasta  -out {folder}AllSeqs.fasta.NCBI.tab -num_threads 8 -max_target_seqs 100 -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovhsp staxids'\".format(folder = folder)\n",
    "#    subprocess.run(command_line, shell=True)\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SILVA 18s rRNA database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SILVA rRNA database project\n",
    "A comprehensive on-line resource for quality checked and aligned ribosomal RNA sequence data.\n",
    "I downloaded 18S rRNA database from the link `https://www.arb-silva.de/fileadmin/silva_databases/release_132/Exports/SILVA_132_LSURef_tax_silva.fasta.gz`\n",
    "\n",
    "There are **150,865** sequences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert to DNA, extract seq length, extract seq head\n",
    "\n",
    "Convert sequences of RNA to DNA.\n",
    "\n",
    "```\n",
    "from Bio import SeqIO\n",
    "f_silva = \"/mnt/d/linux/W/NCBI/SILVA/SILVA_132_LSURef_tax_silva.fasta\"\n",
    "seqs = list(SeqIO.parse(open(f_silva),\"fasta\"))\n",
    "\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA18sDNA.fasta\",\"w\")\n",
    "for seq in seqs:\n",
    "    fout.write(\">\" + seq.description + \"\\n\" + str(seq.seq).upper().replace(\"U\",\"T\")+\"\\n\")\n",
    "fout.close()\n",
    "\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA18sDNA.len\",\"w\")\n",
    "for seq in seqs:\n",
    "    fout.write(seq.id + \"\\t\" + str(len(seq.seq)) + \"\\n\")\n",
    "fout.close()\n",
    "\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA18sDNA.head\",\"w\")\n",
    "for seq in seqs:\n",
    "    fout.write(seq.id + \"\\t\" +seq.description.split(\" \",maxsplit=1)[1] + \"\\n\")\n",
    "fout.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract taxonamy id\n",
    "\n",
    "Write a file with seqID, accession number, length, taxonamy. The separater is \"\\t\"\n",
    "`JQ771801 taxID`. \n",
    "If seqID do not have taxID, write another file\n",
    "`JQ771801 Bacteria;Actinobacteria;Actinobacteria;Streptomycetales;Streptomycetaceae;Streptomyces;Streptomyces sp. Ac105`\n",
    "\n",
    "```\n",
    "from Bio import SeqIO\n",
    "f_silva = \"/mnt/d/linux/W/NCBI/SILVA/SILVA_132_LSURef_tax_silva.fasta\"\n",
    "seqs = list(SeqIO.parse(open(f_silva),\"fasta\"))\n",
    "dcseqs = {e.id.split(\".\")[0]:e for e in seqs}\n",
    "#get the accession ids\n",
    "accs = set([e.id.split(\".\")[0] for e in seqs])\n",
    "print(len(accs))\n",
    "\n",
    "#get a dictionary of seqs\n",
    "dcseqs = {e.id.split(\".\")[0]:e for e in seqs}\n",
    "\n",
    "\n",
    "#creat a dictionary with accession number as key, tax id as value\n",
    "dc_acc2tax = {}\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_gb.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_gss.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_wgs.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_est.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "accsNotInNCBI = [e for e in accs if e not in dc_acc2tax]\n",
    "print(len(accsNotInNCBI))\n",
    "\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA18sDNA.taxa\",\"w\")\n",
    "fout2 = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA18sDNA.taxaRare\",\"w\")\n",
    "for seq_acc in dcseqs:\n",
    "    if seq_acc in dc_acc2tax:\n",
    "        fout.write(seq_acc + \"\\t\" + dc_acc2tax[seq_acc] +\"\\n\")\n",
    "    else:\n",
    "        fout2.write(seq_acc + \"\\t\" + dcseqs[seq_acc].description.split(\" \",maxsplit = 1)[1] + \"\\n\")\n",
    "fout.close()\n",
    "fout2.close()\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build blast db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /mnt/d/linux/W/NCBI/SILVA/\n",
    "/mnt/d/linux/P/blast/bin/dustmasker -in SILVA18sDNA.fasta -infmt fasta -parse_seqids -outfmt maskinfo_asn1_bin -out SILVA18sDNA.asnb\n",
    "\n",
    "/mnt/d/linux/P/blast/bin/makeblastdb -in SILVA18sDNA.fasta -input_type fasta -dbtype nucl -mask_data SILVA18sDNA.asnb -out SILVA18sDNA -title \"SILVA16s\" -parse_seqids -taxid_map SILVA18sDNA.taxa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a file for species with good lineage info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "speicesLineage, only keep those with five elements per line.\n",
    "For taxonamy, speciesName, remove those contain the keywords \"environment\", \"unidentified\", \"uncultured\". Generate a file.\n",
    "of the six terms \"species\tgenus\tfamily\torder\tclass\tphylum\", allow missing two of them.\n",
    "\n",
    "Filename **SILVA18sDNA.goodLineage**\n",
    "\n",
    "```\n",
    "f_Lineage = \"/mnt/d/linux/W/NCBI/taxonamy/speicesLineage\"\n",
    "ls_Lineage = open(f_Lineage).readlines()\n",
    "\n",
    "ls_Lineage = [e.split() for e in ls_Lineage]\n",
    "ls_LineageGood = [e for e in ls_Lineage if len(e) >= 5]\n",
    "ls_LineageGood2 = [e for e in ls_Lineage if len(e) >= 4]\n",
    "\n",
    "ls_LineageGoodSpecies = [e[0] for e in ls_LineageGood2]\n",
    "st_goodSpecies = set(ls_LineageGoodSpecies[1:])\n",
    "\n",
    "f_names = \"/mnt/d/linux/W/NCBI/taxonamy/speciesName\"\n",
    "import re\n",
    "ls_names = open(f_names).readlines()[1:]\n",
    "ls_namesSpecies = [e for e in ls_names if e.split()[0] in st_goodSpecies]\n",
    "ls_goodnames = []\n",
    "noGood = 0\n",
    "for e in ls_namesSpecies:\n",
    "    if re.search(\"environment\", e, re.I) or re.search(\"unidentified\", e, re.I) or re.search(\"uncultured\", e, re.I):\n",
    "        noGood += 1\n",
    "    else:\n",
    "        ls_goodnames.append(e)\n",
    "print(len(ls_goodnames))\n",
    "print(noGood)\n",
    "\n",
    "ls_goodSpecies = [e.split()[0] for e in ls_goodnames]\n",
    "st_goodSpeciesName = set(ls_goodSpecies) #species not with good name and lineage \n",
    "\n",
    "ls_speciesSILVA = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA18sDNA.taxa\").readlines()\n",
    "ls_speciesSILVA = [e.split()[1] for e in ls_speciesSILVA]\n",
    "st_speciesSILVA = set(ls_speciesSILVA)\n",
    "\n",
    "#check keep SILVA species in st_goodSpeciesName\n",
    "st_speciesSILVAgoodLineage = st_speciesSILVA & st_goodSpeciesName\n",
    "#write the lineage info for st_speciesSILVAgoodLineage\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/SILVA/SILVA18sDNA.goodLineage\",\"w\")\n",
    "fout.write(\"\\t\".join([\"taxID\",\"species\",\"genus\",\"family\",\"order\",\"class\",\"phylum\",\"kingdom\"]) +\"\\n\")\n",
    "dc_Lineage = {}\n",
    "lstemp = open(f_Lineage).readlines()[1:]\n",
    "for e in lstemp:\n",
    "    es = e[:-1].split(\"\\t\")\n",
    "    dc_Lineage[es[0]] = es\n",
    "dc_names = {e[:-1].split(\"\\t\")[0]:e[:-1].split(\"\\t\")[1] for e in ls_names}\n",
    "dc_names[\"\"] = \"\"\n",
    "\n",
    "for e in st_speciesSILVAgoodLineage:\n",
    "    _l = dc_Lineage[e]\n",
    "    _lnames = [dc_names[i] for i in _l]\n",
    "    fout.write(\"\\t\".join([e] + _lnames) +\"\\n\")\n",
    "fout.close()    \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungi ITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fasta files were downloaded from `https://unite.ut.ee/repository.php`\n",
    "\n",
    "The header line looks like `Flagelloscypha_japonica|LC146734|SH497095.07FU|reps_singleton| k__Fungi;p__Basidiomycota;c__Agaricomycetes;o__Agaricales;f__Tricholomataceae;g__Flagelloscypha;s__Flagelloscypha_japonica`.\n",
    "\n",
    "check if all headers have 4 \"|\" and 7 \"__\" in name. The answer is Yes.\n",
    "\n",
    "    unique # of the  0 element split by |  20543\n",
    "    unique # of the  1 element split by |  58017\n",
    "    unique # of the  2 element split by |  58048\n",
    "    unique # of the  3 element split by |  4\n",
    "Of the four elements the third one is unique, like \"SH497095.07FU\". Checked some of the 31 accession numbers existing twice in the database, the sequences are identical. Use the accession id as sequence name.  \n",
    "\n",
    "\n",
    "HM123225 = GenBank/UNITE accession number of representative sequence  \n",
    "SH221352.07FU = species hypothesis accession number  \n",
    "UCL7_006587 = compound cluster accession number  \n",
    "refs = this is a manually designated RefS  \n",
    "(reps = this is an automatically chosen RepS)  \n",
    "This specifies the hierarchical classification of the sequence. k = kingdom; p = phylum ; c = class ; o = order ; f = family ; g = genus ; and s = species. Missing information is indicated as \"unidentified\" item; “f__unidentified;” means that no family name for the sequence exists.\n",
    "\n",
    "Extract head. Keep similar format like SILVA.\n",
    "`SH013290.07FU\tFungi;Ascomycota;Leotiomycetes;unidentified;unidentified;unidentified;Leotiomycetes_sp`\n",
    "\n",
    "Extract goodLineage.  \n",
    "`\n",
    "taxID\tspecies\tgenus\tfamily\torder\tclass\tphylum\n",
    "477\tMoraxella lacunata\tMoraxella\tMoraxellaceae\tPseudomonadales\tGammaproteobacteria\tProteobacteria\n",
    "`\n",
    "Here, get taxID from accession ids.\n",
    "\n",
    "```\n",
    "from Bio import SeqIO\n",
    "filename = \"/mnt/d/linux/W/NCBI/fungi/sh_general_release_dynamic_s_01.12.2017_dev.fasta\"\n",
    "ls = list(SeqIO.parse(open(filename),\"fasta\"))\n",
    "\n",
    "# check if all headers have 4 \"|\" and 7 \"__\" in name\n",
    "print(\"number of |\",set([e.id.count(\"|\") for e in ls]))\n",
    "print(\"number of __\", set([e.id.count(\"__\") for e in ls]))\n",
    "\n",
    "for i in range(4):\n",
    "    print(\"unique # of the \",i,\"element split by | \", len(set([e.id.split(\"|\")[i] for e in ls])))\n",
    "\n",
    "# get accession numbers\n",
    "accs = [e.id.split(\"|\")[1] for e in ls]\n",
    "from collections import Counter\n",
    "accs_counter = Counter(accs)\n",
    "accs_counterMorethan1 = {e:accs_counter[e] for e in accs_counter if accs_counter[e] > 1}\n",
    "\n",
    "dc = {e.id.split(\"|\")[1]:e for e in ls}\n",
    "\n",
    "#save the fasta file\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/fungi/ITS7.2.fasta\",\"w\")\n",
    "for _e,e in dc.items():\n",
    "    fout.write(\">\"+e.id.split(\"|\")[1]+\"\\n\"+str(e.seq)+\"\\n\")\n",
    "fout.close()\n",
    "\n",
    "# extract head\n",
    "import re\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/fungi/ITS7.2.head\",\"w\")\n",
    "for e in ls:\n",
    "    _es = e.id.split(\"|\")\n",
    "    _acc = _es[1]\n",
    "    _lineage = re.sub(\"\\w__\", \"\", _es[-1])\n",
    "    _lineage = _lineage.replace(\"_\",\" \")\n",
    "    fout.write(_acc + \"\\t\" + _lineage + \"\\n\")\n",
    "fout.close()\n",
    "\n",
    "# get taxid\n",
    "accs = set(accs)\n",
    "#creat a dictionary with accession number as key, tax id as value\n",
    "dc_acc2tax = {}\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_gb.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_gss.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_wgs.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_est.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "accsNotInNCBI = [e for e in accs if e not in dc_acc2tax]\n",
    "print(len(accsNotInNCBI))\n",
    "\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/fungi/ITS7.2.taxa\",\"w\")\n",
    "fout2 = open(\"/mnt/d/linux/W/NCBI/fungi/ITS7.2.taxaRare\",\"w\")\n",
    "for seq_acc in dc:\n",
    "    if seq_acc in dc_acc2tax:\n",
    "        fout.write(seq_acc + \"\\t\" + dc_acc2tax[seq_acc] +\"\\n\")\n",
    "    else:\n",
    "        fout2.write(seq_acc + \"\\n\")\n",
    "fout.close()\n",
    "fout2.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build blast db\n",
    "```\n",
    "cd /mnt/d/linux/W/NCBI/fungi/\n",
    "/mnt/d/linux/P/blast/bin/dustmasker -in ITS7.2.fasta -infmt fasta -parse_seqids -outfmt maskinfo_asn1_bin -out ITS7.2.asnb\n",
    "\n",
    "/mnt/d/linux/P/blast/bin/makeblastdb -in ITS7.2.fasta -input_type fasta -dbtype nucl -mask_data ITS7.2.asnb -out ITS7.2 -title \"ITS7.2\" -parse_seqids -taxid_map ITS7.2.taxa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build a file for species with good lineage info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "speicesLineage, only keep those with five elements per line.\n",
    "For taxonamy, speciesName, remove those contain the keywords \"environment\", \"unidentified\", \"uncultured\". Generate a file.\n",
    "of the six terms \"species\tgenus\tfamily\torder\tclass\tphylum\", allow missing two of them.\n",
    "\n",
    "Filename **ITS7.2.goodLineage**\n",
    "\n",
    "```\n",
    "f_Lineage = \"/mnt/d/linux/W/NCBI/taxonamy/speicesLineage\"\n",
    "ls_Lineage = open(f_Lineage).readlines()\n",
    "\n",
    "ls_Lineage = [e.split() for e in ls_Lineage]\n",
    "ls_LineageGood = [e for e in ls_Lineage if len(e) >= 5]\n",
    "ls_LineageGood2 = [e for e in ls_Lineage if len(e) >= 4]\n",
    "\n",
    "ls_LineageGoodSpecies = [e[0] for e in ls_LineageGood2]\n",
    "st_goodSpecies = set(ls_LineageGoodSpecies[1:])\n",
    "\n",
    "f_names = \"/mnt/d/linux/W/NCBI/taxonamy/speciesName\"\n",
    "import re\n",
    "ls_names = open(f_names).readlines()[1:]\n",
    "ls_namesSpecies = [e for e in ls_names if e.split()[0] in st_goodSpecies]\n",
    "ls_goodnames = []\n",
    "noGood = 0\n",
    "for e in ls_namesSpecies:\n",
    "    if re.search(\"environment\", e, re.I) or re.search(\"unidentified\", e, re.I) or re.search(\"uncultured\", e, re.I):\n",
    "        noGood += 1\n",
    "    else:\n",
    "        ls_goodnames.append(e)\n",
    "print(len(ls_goodnames))\n",
    "print(noGood)\n",
    "\n",
    "ls_goodSpecies = [e.split()[0] for e in ls_goodnames]\n",
    "st_goodSpeciesName = set(ls_goodSpecies) #species not with good name and lineage \n",
    "\n",
    "ls_speciesSILVA = open(\"/mnt/d/linux/W/NCBI/fungi/ITS7.2.taxa\").readlines()\n",
    "ls_speciesSILVA = [e.split()[1] for e in ls_speciesSILVA]\n",
    "st_speciesSILVA = set(ls_speciesSILVA)\n",
    "\n",
    "#check keep SILVA species in st_goodSpeciesName\n",
    "st_speciesSILVAgoodLineage = st_speciesSILVA & st_goodSpeciesName\n",
    "#write the lineage info for st_speciesSILVAgoodLineage\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/fungi/ITS7.2.goodLineage\",\"w\")\n",
    "fout.write(\"\\t\".join([\"taxID\",\"species\",\"genus\",\"family\",\"order\",\"class\",\"phylum\",\"kingdom\"]) +\"\\n\")\n",
    "dc_Lineage = {}\n",
    "lstemp = open(f_Lineage).readlines()[1:]\n",
    "for e in lstemp:\n",
    "    es = e[:-1].split(\"\\t\")\n",
    "    dc_Lineage[es[0]] = es\n",
    "dc_names = {e[:-1].split(\"\\t\")[0]:e[:-1].split(\"\\t\")[1] for e in ls_names}\n",
    "dc_names[\"\"] = \"\"\n",
    "\n",
    "for e in st_speciesSILVAgoodLineage:\n",
    "    _l = dc_Lineage[e]\n",
    "    _lnames = [dc_names[i] for i in _l]\n",
    "    fout.write(\"\\t\".join([e] + _lnames) +\"\\n\")\n",
    "fout.close()   \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ezbio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### basic info\n",
    "\n",
    "**62,240** seqs. \n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5563544/\n",
    "\n",
    "Introducing EzBioCloud: a taxonomically united database of 16S rRNA gene sequences and whole-genome assemblies\n",
    "\n",
    "Since everything is well prepared for this database. Build the database directly.\n",
    "\n",
    "**14,849** of them have taxonamy ids, about 25%.\n",
    "\n",
    "Get the taxa ids for those sequences\n",
    "```\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "filename = \"/mnt/d/linux/W/NCBI/Ezbio/eztaxon_qiime_full.fasta\"\n",
    "from Bio import SeqIO\n",
    "lsEzfasta = list(SeqIO.parse(open(filename),\"fasta\"))\n",
    "\n",
    "dcName2tax = defaultdict()\n",
    "with open(\"/mnt/d/linux/W/NCBI/taxonamy/names.dmp\") as f:\n",
    "    _ls = f.readlines()\n",
    "for _l in _ls:\n",
    "    _e = _l.split(\"\\t\")\n",
    "    _k = _e[2]\n",
    "    _k = _k.upper()\n",
    "    _v = _e[0]\n",
    "    dcName2tax[_k] = _v\n",
    "#    break\n",
    "\n",
    "filename = \"/mnt/d/linux/W/NCBI/Ezbio/eztaxon_id_taxonomy.txt\"\n",
    "with open(filename) as f:\n",
    "    ls3 = f.readlines()\n",
    "dc_Ez = {}\n",
    "for _l in ls3:\n",
    "    _k,_v = _l.split()\n",
    "    _v = _v.split(\";\")[-1].replace(\"_\",\" \").upper()\n",
    "    dc_Ez[_v] = _k\n",
    "\n",
    "df_Ez = pd.DataFrame(data = {\"name\":list(dc_Ez.keys()),\"seqid\":list(dc_Ez.values())})\n",
    "\n",
    "df_Ez[\"taxID\"] = df_Ez[\"name\"].apply(lambda x:dcName2tax.get(x))\n",
    "\n",
    "df_Ez_tax = df_Ez[df_Ez[\"taxID\"].notnull()]\n",
    "df_Ez_noTax = df_Ez[df_Ez[\"taxID\"].isnull()]\n",
    "\n",
    "df_Ez_tax.to_csv(\"/mnt/d/linux/W/NCBI/Ezbio/eztaxon_qiime_full.taxa\", sep=\"\\t\", columns=[\"seqid\",\"taxID\"], header=False, index=False)\n",
    "df_Ez_noTax.to_csv(\"/mnt/d/linux/W/NCBI/Ezbio/NoTax\", sep=\"\\t\")\n",
    "\n",
    "df_goodLineage = pd.read_csv(\"/mnt/d/linux/W/NCBI/taxonamy/speicesLineageName\", sep = \"\\t\", dtype = str)\n",
    "df_goodLineage.index = df_goodLineage[\"taxID\"]\n",
    "df_goodLineage = df_goodLineage.drop(labels=[\"taxID\"], axis = 1)\n",
    "df_temp = df_Ez_tax.copy()\n",
    "df_temp = df_temp.join(df_goodLineage,on='taxID',how='left')\n",
    "df_temp.to_csv(\"/mnt/d/linux/W/NCBI/Ezbio/eztaxon_qiime_full.goodLineage\", sep=\"\\t\", columns=['taxID', 'species', 'genus', 'family', 'order','class', 'phylum', 'kingdom'], header=True, index=False)\n",
    "\n",
    "```\n",
    "\n",
    "add lineage info for '85620 264669 99810 497816' manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build blastdb\n",
    "```\n",
    "cd /mnt/d/linux/W/NCBI/Ezbio/\n",
    "/mnt/d/linux/P/blast/bin/dustmasker -in eztaxon_qiime_full.fasta -infmt fasta -parse_seqids -outfmt maskinfo_asn1_bin -out eztaxon_qiime_full.asnb\n",
    "\n",
    "/mnt/d/linux/P/blast/bin/makeblastdb -in eztaxon_qiime_full.fasta -input_type fasta -dbtype nucl -mask_data eztaxon_qiime_full.asnb -out eztaxon_qiime_full -title \"Ezbio\" -parse_seqids -taxid_map eztaxon_qiime_full.taxa\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build the pipeline\n",
    "An updated pipeline with choices for db\n",
    "```\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Mar 30 13:35:56 2018\n",
    "\n",
    "@author: x\n",
    "\"\"\"\n",
    "\n",
    "folder = \"/mnt/d/linux/W/MoonNt/KC/\"\n",
    "error_rate = 0.005\n",
    "\n",
    "import glob\n",
    "import re\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "def getSeqFiles(folder):\n",
    "    '''\n",
    "    given a foler name, return a list of all sequence files.\n",
    "    seq files are files end with .seq\n",
    "    '''\n",
    "    files = glob.glob(folder + '**/*.seq', recursive=True)\n",
    "    return files\n",
    "\n",
    "def getFastaFromFile(filename):\n",
    "    '''\n",
    "    given a filename, return a str of fasta format.\n",
    "    some files contain extra info\n",
    "    filename looks like '/mnt/d/linux/W/MoonNt/KC/180223 SEQ/MN210615.seq'\n",
    "    return a string of fasta format\n",
    "    '''\n",
    "    fasta_str = \">\" + re.split(\"\\\\\\\\|/\",filename)[-1].split(\".\")[0] + \"\\n\"\n",
    "    txt_ls = open(filename).read().split(\"\\n\")\n",
    "    if txt_ls[0][0] != \">\":\n",
    "        seq = [e for e in txt_ls if re.match(\"[ATCGN]+\\n?$\",e,re.I)]\n",
    "        if len(seq) == 0:\n",
    "            print(filename, \"something is wrong\")\n",
    "            return \"\"\n",
    "        return fasta_str + ''.join(seq) + \"\\n\"\n",
    "    return fasta_str + str(SeqIO.read(open(filename),format = \"fasta\").seq) + \"\\n\"\n",
    "\n",
    "def taxaFinder(folder = folder):\n",
    "    '''\n",
    "    given a path of a folder, output a file with the alignment and taxa info for the sequences\n",
    "    '''\n",
    "    # add the slash to make the path easier to use\n",
    "    if folder[-1] != \"/\":\n",
    "        folder = folder + \"/\"\n",
    "    \n",
    "    # get paths of all sequence files\n",
    "    files = getSeqFiles(folder)\n",
    "    \n",
    "    # get all fasta sequences\n",
    "    fasta_seqs = [getFastaFromFile(e) for e in files]\n",
    "    folder = folder + folder.split(\"/\")[-2]+\"_\"\n",
    "    # save the fasta_seqs to a file in folder\n",
    "    open(folder + \"AllSeqs.fasta\",\"w\").write(\"\".join(fasta_seqs))\n",
    "    \n",
    "    # run local blast against SILVA\n",
    "    command_line = \"/mnt/d/linux/P/blast/bin/blastn -db /mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA -query {folder}AllSeqs.fasta  -out {folder}AllSeqs.fasta.SILVA.tab -num_threads 8 -max_target_seqs 100 -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovhsp staxids'\".format(folder = folder)\n",
    "    subprocess.run(command_line, shell=True)\n",
    "    \n",
    "    # read in blast result for SILVA\n",
    "    df_SILVA = pd.read_csv(\"{folder}AllSeqs.fasta.SILVA.tab\".format(folder = folder), sep = \"\\t\",header = None, dtype = {0:str,1:str,2:float,3:int,4:int,5:int,6:int,7:int,8:int,9:int,10:float,11:float,12:int,13:str})\n",
    "    df_SILVA.columns = [\"query\", \"subject\",\"identity\",\"matchLength\",\"missMatch\",\"gap\",\"qstart\",\"qend\",\"sstart\", \"send\", \"evalue\", \"bitScore\", \"qcover\", \"taxID\"]\n",
    "    ## add column 14, length of identical bases\n",
    "    df_SILVA[\"identical\"] = round(df_SILVA[\"identity\"] * df_SILVA[\"matchLength\"]/100)\n",
    "    ## for subject id, only keep accession number\n",
    "    df_SILVA[\"subject\"] = df_SILVA[\"subject\"].apply(lambda x: x.split(\".\")[0])\n",
    "    ## sort by query id and identical bases\n",
    "#    df_SILVA = df_SILVA.sort_values(by = [0,14],axis = 0, ascending=[True, False])\n",
    "    ## get max identical bases for each query\n",
    "#    query_max = df_SILVA.groupby(by = [0])[14].max()\n",
    "#    query_max = query_max.to_dict()\n",
    "#    ## add column 15, allow the identical bases to with the error of 0.5% compare to query_max\n",
    "#    df_SILVA[15] = df_SILVA.apply(lambda x:(query_max[x[0]] - x[14])/query_max[x[0]] <= error_rate, axis = 1)\n",
    "#    ## identical bases True \n",
    "#    df_SILVA_filter1 = df_SILVA[df_SILVA[15]]\n",
    "    \n",
    "    ## add a column, to filter identity > 97, qcover > 95\n",
    "    df_SILVA[\"filter1\"] = (df_SILVA[\"identity\"] > 97) & (df_SILVA[\"qcover\"] > 95)\n",
    "    \n",
    "    df_Lineage = pd.read_csv(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.head\", sep = \"\\t\", header = None, dtype = str)\n",
    "    df_Lineage.columns = [\"acc\",\"SILVAtaxa\"]\n",
    "    df_Lineage[\"acc\"] = df_Lineage[\"acc\"].apply(lambda x:x.split(\".\")[0])\n",
    "    df_Lineage.index = df_Lineage[\"acc\"]\n",
    "    df_Lineage = df_Lineage.drop(labels=[\"acc\"], axis = 1)\n",
    "    \n",
    "    df_SILVA = df_SILVA.join(df_Lineage, on=\"subject\",how=\"left\")\n",
    "    \n",
    "    \n",
    "    # read in SILVA16sDNA.goodLineage as dataframe\n",
    "    df_goodLineage = pd.read_csv(\"/mnt/d/linux/W/NCBI/SILVA/SILVA16sDNA.goodLineage\", sep = \"\\t\", dtype = str)\n",
    "    df_goodLineage.index = df_goodLineage[\"taxID\"]\n",
    "    df_goodLineage = df_goodLineage.drop(labels=[\"taxID\"], axis = 1)\n",
    "    \n",
    "    df_SILVA = df_SILVA.join(df_goodLineage, on=\"taxID\",how=\"left\")\n",
    "    df_SILVA.loc[:,['query', 'subject', 'identity', 'matchLength', 'qcover',\n",
    "       'taxID', 'identical', 'species', 'genus',\n",
    "       'family', 'order', 'class', 'phylum', 'SILVAtaxa']].to_excel(folder + \"AllSILVAresult.xlsx\", index = False)\n",
    "    \n",
    "    df_SILVA_keep1 = df_SILVA[df_SILVA[\"filter1\"] & df_SILVA[\"species\"] & df_SILVA[\"taxID\"]].drop_duplicates(subset = [\"query\"], keep = \"first\").copy()\n",
    "    \n",
    "    df_SILVA_keep1.loc[:,['query', 'subject', 'identity', 'matchLength', 'qcover',\n",
    "       'taxID', 'identical', 'species', 'genus',\n",
    "       'family', 'order', 'class', 'phylum', 'SILVAtaxa']].to_excel(folder + \"BestSILVAresult.xlsx\", index = False)\n",
    "    \n",
    "    queries = set(df_SILVA_keep1[\"query\"])\n",
    "    fout = open(folder + \"sequencesNotIdentified.fasta\",\"w\")\n",
    "    for seq in fasta_seqs:\n",
    "        _id = seq.split()[0][1:]\n",
    "        if _id not in queries:\n",
    "            fout.write(seq)\n",
    "    fout.close()\n",
    "    \n",
    "#    command_line = \"/mnt/d/linux/P/blast/bin/blastn -db /mnt/d/linux/W/NCBI/nt_rRNA/rRNA2 -query {folder}AllSeqs.fasta  -out {folder}AllSeqs.fasta.NCBI.tab -num_threads 8 -max_target_seqs 100 -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovhsp staxids'\".format(folder = folder)\n",
    "#    subprocess.run(command_line, shell=True)\n",
    "\n",
    "def taxaFinderEzbio(folder = folder):\n",
    "    '''\n",
    "    given a path of a folder, output a file with the alignment and taxa info for the sequences\n",
    "    '''\n",
    "    # add the slash to make the path easier to use\n",
    "    if folder[-1] != \"/\":\n",
    "        folder = folder + \"/\"\n",
    "    \n",
    "    # get paths of all sequence files\n",
    "    files = getSeqFiles(folder)\n",
    "    \n",
    "    # get all fasta sequences\n",
    "    fasta_seqs = [getFastaFromFile(e) for e in files]\n",
    "    folder = folder + folder.split(\"/\")[-2]+\"_\"\n",
    "    # save the fasta_seqs to a file in folder\n",
    "    open(folder + \"AllSeqs.fasta\",\"w\").write(\"\".join(fasta_seqs))\n",
    "    \n",
    "    # run local blast against SILVA\n",
    "    command_line = \"/mnt/d/linux/P/blast/bin/blastn -db /mnt/d/linux/W/NCBI/Ezbio/eztaxon_qiime_full -query {folder}AllSeqs.fasta  -out {folder}AllSeqs.fasta.Ezbio.tab -num_threads 8 -max_target_seqs 100 -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovhsp staxids'\".format(folder = folder)\n",
    "    subprocess.run(command_line, shell=True)\n",
    "    \n",
    "    # read in blast result for SILVA\n",
    "    df_Ezbio = pd.read_csv(\"{folder}AllSeqs.fasta.Ezbio.tab\".format(folder = folder), sep = \"\\t\",header = None, dtype = {0:str,1:str,2:float,3:int,4:int,5:int,6:int,7:int,8:int,9:int,10:float,11:float,12:int,13:str})\n",
    "    df_Ezbio.columns = [\"query\", \"subject\",\"identity\",\"matchLength\",\"missMatch\",\"gap\",\"qstart\",\"qend\",\"sstart\", \"send\", \"evalue\", \"bitScore\", \"qcover\", \"taxID\"]\n",
    "#    df_Ezbio['taxID'] = df_Ezbio[\"subject\"]\n",
    "    ## add column 14, length of identical bases\n",
    "    df_Ezbio[\"identical\"] = round(df_Ezbio[\"identity\"] * df_Ezbio[\"matchLength\"]/100)\n",
    "    \n",
    "    ## add a column, to filter identity > 97, qcover > 95\n",
    "    df_Ezbio[\"filter1\"] = (df_Ezbio[\"identity\"] > 97) & (df_Ezbio[\"qcover\"] > 95)\n",
    "    \n",
    "    df_Lineage = pd.read_csv(\"/mnt/d/linux/W/NCBI/Ezbio/eztaxon_id_taxonomy.txt\", sep = \"\\t\", header = None, dtype = str)\n",
    "    df_Lineage.columns = [\"acc\",\"EzbioTaxa\"]\n",
    "    df_Lineage[\"EzbioTaxa\"] = df_Lineage[\"EzbioTaxa\"].apply(lambda x:x.replace(\"_\",\" \"))\n",
    "    df_Lineage.index = df_Lineage[\"acc\"]\n",
    "    df_Lineage = df_Lineage.drop(labels=[\"acc\"], axis = 1)\n",
    "    \n",
    "    df_Ezbio = df_Ezbio.join(df_Lineage, on=\"subject\",how=\"left\")\n",
    "    \n",
    "    \n",
    "    # read in SILVA16sDNA.goodLineage as dataframe\n",
    "    df_goodLineage = df_Lineage.copy()\n",
    "    _l = df_goodLineage[\"EzbioTaxa\"].str.split(\";\")\n",
    "    df_goodLineage[\"kingdom\"], df_goodLineage[\"phylum\"], df_goodLineage[\"class\"], df_goodLineage[\"order\"], df_goodLineage[\"family\"],  df_goodLineage[\"genus\"], df_goodLineage[\"species\"] = _l.str\n",
    "    df_goodLineage = df_goodLineage.drop(labels=[\"EzbioTaxa\"], axis = 1)\n",
    "    \n",
    "    df_Ezbio = df_Ezbio.join(df_goodLineage, on=\"subject\",how=\"left\")\n",
    "    df_Ezbio.loc[:,['query', 'subject', 'identity', 'matchLength', 'qcover',\n",
    "       'taxID', 'identical', 'species', 'genus',\n",
    "       'family', 'order', 'class', 'phylum', 'EzbioTaxa']].to_excel(folder + \"AllEzbioResult.xlsx\", index = False)\n",
    "    \n",
    "    df_Ezbio_keep1 = df_Ezbio[df_Ezbio[\"filter1\"] & df_Ezbio[\"species\"] & df_Ezbio[\"taxID\"]].drop_duplicates(subset = [\"query\"], keep = \"first\").copy()\n",
    "    \n",
    "    df_Ezbio_keep1.loc[:,['query', 'subject', 'identity', 'matchLength', 'qcover','taxID', 'identical', 'species', 'genus','family', 'order', 'class', 'phylum', 'EzbioTaxa']].to_excel(folder + \"BestEzbioResult.xlsx\", index = False)\n",
    "    \n",
    "    queries = set(df_Ezbio_keep1[\"query\"])\n",
    "    fout = open(folder + \"sequencesNotIdentifiedEzbio.fasta\",\"w\")\n",
    "    for seq in fasta_seqs:\n",
    "        _id = seq.split()[0][1:]\n",
    "        if _id not in queries:\n",
    "            fout.write(seq)\n",
    "    fout.close()\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='the folder path of sequencing results')\n",
    "parser.add_argument('folder',help = 'input folder path of sequencing results')\n",
    "parser.add_argument(\"--db\", \"-d\", help = \"database to use, default SILVA\", default = \"SILVA\")\n",
    "f = parser.parse_args()\n",
    "folder = f.folder\n",
    "if f.db == \"SILVA\":\n",
    "    taxaFinder(folder)\n",
    "elif f.db == \"Ezbio\":\n",
    "    taxaFinderEzbio(folder)\n",
    "else:\n",
    "    print(\"undefined database\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nt reference with sequences with good lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract taxid from nt\n",
    "\n",
    "nt.tax, accession and tax id\n",
    "```\n",
    "f_nt = \"/mnt/d/linux/W/NCBI/nt/nt\"\n",
    "from Bio import SeqIO\n",
    "seqs = SeqIO.parse(open(f_nt), \"fasta\")\n",
    "accs = []\n",
    "for seq in seqs:\n",
    "    accs.append(seq.id.split(\".\")[0])\n",
    "print(len(accs))\n",
    "\n",
    "# get taxid\n",
    "#accs = set(accs)\n",
    "#creat a dictionary with accession number as key, tax id as value\n",
    "dc_acc2tax = dict.fromkeys(accs)\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_gb.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_gss.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_wgs.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "f_acc2tax = \"/mnt/d/linux/W/NCBI//taxonamy/nucl_est.accession2taxid\"\n",
    "fo = open(f_acc2tax)\n",
    "fo.readline()\n",
    "for l in fo:\n",
    "    es = l.split()\n",
    "    if es[0] in accs:\n",
    "        dc_acc2tax[es[0]] = es[2]\n",
    "fo.close()\n",
    "print(len(dc_acc2tax))\n",
    "\n",
    "\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/nt/nt.taxa\",\"w\")\n",
    "fout2 = open(\"/mnt/d/linux/W/NCBI/nt/nt.taxaRare\",\"w\")\n",
    "for _k,_v in dc_acc2tax.items():\n",
    "    if _v:\n",
    "        fout.write(_k + \"\\t\" + _v +\"\\n\")\n",
    "    else:\n",
    "        fout2.write(_k + \"\\n\")\n",
    "fout.close()\n",
    "fout2.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract sequences with good taxonamy\n",
    "\n",
    "Only include those from kingdoms of Archae, Bacteria, Viroids, Viruses and Fungi.\n",
    "\n",
    "Also, get **goodLineage**, length.\n",
    "\n",
    "```\n",
    "\n",
    "f_Lineage = \"/mnt/d/linux/W/NCBI/taxonamy/speicesLineage\"\n",
    "import pandas as pd\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "df_lineage = pd.read_csv(f_Lineage,sep = \"\\t\", dtype = str)\n",
    "df_lineage[\"taxaCount\"] = df_lineage.apply(lambda x:7-sum(x.isnull()), axis = 1)\n",
    "df_lineageFilter = df_lineage[df_lineage[\"taxaCount\"] >=5]\n",
    "\n",
    "species = set(df_lineageFilter[\"species\"])\n",
    "\n",
    "f_names = \"/mnt/d/linux/W/NCBI/taxonamy/speciesName\"\n",
    "df_names = pd.read_csv(f_names,sep=\"\\t\",dtype=str)\n",
    "df_names[\"good\"] = df_names[\"name\"].apply(lambda e:False if re.search(\"environment\", e, re.I) or re.search(\"unidentified\", e, re.I) or re.search(\"uncultured\", e, re.I) else True)\n",
    "\n",
    "speciesGoodname = set(df_names[df_names[\"good\"]][\"id\"])\n",
    "\n",
    "speciesGoodLineageName = species & speciesGoodname\n",
    "\n",
    "df_lineageFilter[\"select\"] = df_lineageFilter[\"kingdom\"].apply(lambda x: x in {\"2157\",\"2\",\"12884\",\"10239\",\"4751\"})\n",
    "speciesKingdom = set(df_lineageFilter[df_lineageFilter[\"select\"]][\"species\"])\n",
    "speciesGoodLineageNameKingdom = speciesGoodLineageName & speciesKingdom\n",
    "\n",
    "accessionGood = []\n",
    "f = \"/mnt/d/linux/W/NCBI/nt/nt.taxa\"\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/nt_ref/nt_micro.taxa\",\"w\")\n",
    "for line in open(f):\n",
    "    _acc, _species = line.split()\n",
    "    if _species in speciesGoodLineageNameKingdom:\n",
    "        accessionGood.append(_acc)\n",
    "        fout.write(_acc + \"\\t\" + _species+\"\\n\")\n",
    "accessionGoodst = set(accessionGood)\n",
    "fout.close()\n",
    "\n",
    "#get fasta with acc in accessionGoodst\n",
    "\n",
    "f = \"/mnt/d/linux/W/NCBI/nt/nt\"\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/nt_ref/nt_micro\",\"w\")\n",
    "for _s in SeqIO.parse(open(f),format=\"fasta\"):\n",
    "    if _s.id.split(\".\")[0] in accessionGoodst:\n",
    "        fout.write(\">\"+_s.description+\"\\n\"+str(_s.seq) + \"\\n\")\n",
    "fout.close()\n",
    "\n",
    "# seq length\n",
    "fout = open(\"/mnt/d/linux/W/NCBI/nt_ref/nt_micro.len\",\"w\")\n",
    "with open(\"/mnt/d/linux/W/NCBI/nt_ref/nt_micro\") as f:\n",
    "    for _s in SeqIO.parse(f,format=\"fasta\"):\n",
    "        fout.write(_s.id +\"\\t\" + str(len(_s.seq))+\"\\n\")\n",
    "fout.close()\n",
    "\n",
    "# goodLineage\n",
    "df_lineage[\"keep\"] = df_lineage[\"species\"].apply(lambda x:x in speciesGoodLineageNameKingdom)\n",
    "df_lineageKeep = df_lineage[df_lineage[\"keep\"]]\n",
    "print(df_lineageKeep.shape)\n",
    "\n",
    "file_name = \"/mnt/d/linux/W/NCBI/taxonamy/names.dmp\"\n",
    "import pandas as pd\n",
    "df_name = pd.read_csv(file_name,sep = \"\\t\", header = None,dtype = str)\n",
    "df_name = df_name.loc[:,[0,2,6]]\n",
    "df_name_keep = df_name[df_name[6] == \"scientific name\"]\n",
    "dcName = dict(zip(df_name_keep[0],df_name_keep[2]))\n",
    "\n",
    "\n",
    "df_lineageKeepName = df_lineageKeep.copy()\n",
    "df_lineageKeepName = df_lineageKeepName.drop(labels = [\"keep\", \"taxaCount\"],axis=1)\n",
    "df_lineageKeepName = df_lineageKeepName.applymap(lambda x:dcName[x] if x in dcName else \"\")\n",
    "df_lineageKeepName[\"taxID\"] = df_lineageKeep[\"species\"]\n",
    "\n",
    "df_lineageKeepName.to_csv(\"/mnt/d/linux/W/NCBI/nt_ref/nt_micro.goodLineage\",columns = [\"taxID\"] + list(df_lineageKeepName.columns[:-1]), sep=\"\\t\",index = None)\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build database for nt_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd /mnt/d/linux/W/NCBI/nt_ref/\n",
    "/mnt/d/linux/P/blast/bin/dustmasker -in nt_micro -infmt fasta -parse_seqids -outfmt maskinfo_asn1_bin -out nt_micro.asnb\n",
    "\n",
    "/mnt/d/linux/P/blast/bin/makeblastdb -in nt_micro -input_type fasta -dbtype nucl -mask_data nt_micro.asnb -out nt_micro -title \"nt_micro\" -parse_seqids -taxid_map nt_micro.taxa\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nr database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download nr from ncbi\n",
    "\n",
    "```\n",
    "l = [\"/mnt/d/linux/.aspera/connect/bin/ascp -T -k 1 -i ~/.aspera/connect/etc/asperaweb_id_dsa.openssh anonftp@ftp.ncbi.nlm.nih.gov:/blast/db/nr.%02d.tar.gz  ~/W/NCBI/nr201804/\"%i for i in range(85)]\n",
    "import subprocess\n",
    "for commandline in l:\n",
    "    subprocess.run(commandline,shell=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VFDB\n",
    "\n",
    "VFDB is virulence factor database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VFDB nt, get accession numbers and taxid\n",
    "\n",
    "blast against nt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "/mnt/d/linux/P/blast/bin/blastn -db /mnt/d/linux/W/NCBI/nt_ref/nt_micro -query /mnt/d/linux/W/NCBI/VFDB/VFDB_setB_nt.fas -out /mnt/d/linux/W/NCBI/VFDB/VFDB_setB_nt.tab -num_threads 8 -max_target_seqs 10 -word_size 56 -outfmt '6 qseqid sseqid pident length mismatch gapopen qstart qend sstart send evalue bitscore qcovhsp staxids'\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VFDB protein, build blast database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rename proteins\n",
    "```\n",
    "from Bio import SeqIO\n",
    "filename = '/mnt/d/linux/W/NCBI/VFDB/VFDB_setB_pro.fas'\n",
    "fileout = '/mnt/d/linux/W/NCBI/VFDB/VFDB_blast201806/VFDB_Pr'\n",
    "lis_pr = list(SeqIO.parse(open(filename),'fasta'))\n",
    "fout = open(fileout,'w')\n",
    "# use VFG id as seq name, and extract other information\n",
    "for _l in lis_pr:\n",
    "    _name = _l.id[:9] +' '+ _l.description.split(' ',1)[1]\n",
    "    fout.write('>'+_name+'\\n'+str(_l.seq)+'\\n')\n",
    "fout.close()\n",
    "```\n",
    "build database for blast search\n",
    "```\n",
    "cd /mnt/d/linux/W/NCBI/VFDB/VFDB_blast201806\n",
    "/mnt/d/linux/P/blast/bin/dustmasker -in VFDB_Pr -infmt fasta -parse_seqids -outfmt maskinfo_asn1_bin -out VFDB_Pr.asnb\n",
    "\n",
    "/mnt/d/linux/P/blast/bin/makeblastdb -in VFDB_Pr -input_type fasta -dbtype prot -mask_data VFDB_Pr.asnb -out VFDB_Pr -title \"VFDB_Pr\" -parse_seqids \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract information, VF id, from the file\n",
    "```\n",
    "from Bio import SeqIO\n",
    "import re\n",
    "filename = '/mnt/d/linux/W/NCBI/VFDB/VFDB_blast201806/VFDB_Pr'\n",
    "file_head = filename +'.head'\n",
    "file_VFs = filename + '.VF'\n",
    "lis = list(SeqIO.parse(open(filename),'fasta'))\n",
    "\n",
    "fout = open(file_head,'w')\n",
    "fout.write('VFDB_id\\tVFDB_description\\n')\n",
    "for _l in lis:\n",
    "    fout.write('\\t'.join(_l.description.split(' ',1)) + '\\n')\n",
    "fout.close()\n",
    "\n",
    "fout = open(file_VFs, 'w')\n",
    "fout.write('VFDB_id\\tVF_id\\n')\n",
    "for _l in lis:\n",
    "    _VF = re.findall('\\(C*VF\\d*\\)',_l.description)\n",
    "    if len(_VF)>0:\n",
    "        _VF = _VF[0]\n",
    "        _VF = _VF[1:-1]\n",
    "        fout.write(_l.id+'\\t'+_VF+'\\n')\n",
    "fout.close()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "335px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
